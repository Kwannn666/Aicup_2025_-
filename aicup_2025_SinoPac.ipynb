{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e916a3c-5261-4d3c-a8bc-cc4f8fefb432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 正在清除讀入的資料 DataFrame...\n",
      "✅ 清除完成，可重新執行資料載入程式\n"
     ]
    }
   ],
   "source": [
    "# === 🧹 清除 train_df / test_df 等相關變數與釋放記憶體 ===\n",
    "import gc\n",
    "\n",
    "print(\"🧹 正在清除讀入的資料 DataFrame...\")\n",
    "\n",
    "# 確保變數存在才刪除，避免報錯\n",
    "if 'train_df' in locals():\n",
    "    del train_df\n",
    "if 'test_df' in locals():\n",
    "    del test_df\n",
    "\n",
    "# 嘗試釋放記憶體\n",
    "gc.collect()\n",
    "\n",
    "print(\"✅ 清除完成，可重新執行資料載入程式\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "841c861d-d013-4b9d-9c58-3bf383fc2aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 正在讀取 training.csv ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading training.csv: 100%|██████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 65.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 正在讀取 public_x.csv ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading public_x.csv: 100%|██████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 64.61it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# === Step 1: 模擬讀取資料進度條 ===\n",
    "print(\"📂 正在讀取 training.csv ...\")\n",
    "for _ in tqdm(range(100), desc=\"Loading training.csv\"):\n",
    "    time.sleep(0.002)\n",
    "\n",
    "train_df = pd.read_csv(\"training.csv\")\n",
    "\n",
    "print(\"📂 正在讀取 public_x.csv ...\")\n",
    "for _ in tqdm(range(100), desc=\"Loading public_x.csv\"):\n",
    "    time.sleep(0.002)\n",
    "\n",
    "test_df = pd.read_csv(\"public_x.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5addfc1f-c727-4958-bb67-eebf6668acbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 清除之前模型的訓練結果和變數...\n",
      "🔄 重置完成，可以重新進行模型訓練\n"
     ]
    }
   ],
   "source": [
    "# === 🧹 清除訓練結果並重設環境（保留資料） ===\n",
    "import gc\n",
    "\n",
    "print(\"🧹 清除之前模型的訓練結果和變數...\")\n",
    "for var in ['X', 'y', 'X_train', 'X_test', 'y_train', 'y_test', 'X_train', 'y_train',\n",
    "            'model', 'best_model', 'grid_search', 'shap_values', 'shap_importance']:\n",
    "    if var in globals():\n",
    "        del globals()[var]\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print(\"🔄 重置完成，可以重新進行模型訓練\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22bccef4-4d7c-4b11-b0c6-a00ccce88182",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11756\\1836817306.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# === 📥 載入資料 ===\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, RandomizedSearchCV\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, f1_score\n",
    "import shap\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['font.family'] = 'Microsoft JhengHei'  # 支援中文字型\n",
    "\n",
    "# === 📥 載入資料 ===\n",
    "X = train_df[features].replace([np.inf, -np.inf], np.nan).fillna(0).astype(np.float32)\n",
    "y = y.astype(int)\n",
    "\n",
    "# === ✂️ 切分訓練 / 測試資料 ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# === ⚖️ 計算類別權重 ===\n",
    "print(\"⚖️ 計算類別權重...\")\n",
    "scale_pos_weight = y_train.value_counts()[0] / y_train.value_counts()[1]\n",
    "\n",
    "# === 📈 初始 LightGBM 模型訓練（for SHAP） ===\n",
    "print(\"📈 訓練初始 LightGBM 模型（for SHAP 分析）...\")\n",
    "model = LGBMClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=10,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    device='gpu',\n",
    "    boosting_type='gbdt',\n",
    "    objective='binary',\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# === 🔍 計算 SHAP 值 ===\n",
    "print(\"🔍 計算 SHAP 值以篩選重要特徵...\")\n",
    "X_sample = X_train.sample(n=8000, random_state=42)\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "# 安全檢查 shap_values 是否為 list（新版 LightGBM 有可能是 list，也可能是 array）\n",
    "if isinstance(shap_values, list):\n",
    "    shap_vals = shap_values[1]  # 正類\n",
    "else:\n",
    "    shap_vals = shap_values     # 若不是 list 就直接使用\n",
    "\n",
    "shap_importance = pd.DataFrame({\n",
    "    'feature': X_sample.columns,\n",
    "    'mean_abs_shap': np.abs(shap_vals).mean(axis=0)\n",
    "}).sort_values(by='mean_abs_shap', ascending=False)\n",
    "\n",
    "# 繪圖\n",
    "shap.summary_plot(shap_vals, X_sample, max_display=30)\n",
    "\n",
    "# 匯出重要特徵\n",
    "top_features = shap_importance.head(500)['feature'].tolist()\n",
    "shap_importance.head(100).to_csv(\"shap_top_features.csv\", index=False)\n",
    "\n",
    "# === 🔧 RandomizedSearchCV 調參 ===\n",
    "print(\"🎯 開始 RandomizedSearchCV 調參...\")\n",
    "param_dist = {\n",
    "    'n_estimators': [600, 800, 1000, 1200],\n",
    "    'max_depth': [8, 10, 12, 20],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "    'reg_alpha': [0, 0.1],\n",
    "    'reg_lambda': [1, 5, 10],\n",
    "    'min_data_in_leaf': [10, 20, 50],\n",
    "    'min_gain_to_split': [0, 0.001, 0.01],\n",
    "    'max_bin': [255],\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=LGBMClassifier(\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        objective='binary',\n",
    "        device='gpu',\n",
    "        boosting_type='gbdt',\n",
    "        random_state=42\n",
    "    ),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,\n",
    "    scoring='f1',\n",
    "    cv=3,\n",
    "    verbose=3,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "random_search.fit(X_train[top_features], y_train)\n",
    "\n",
    "print(\"✅ 調參完成！\")\n",
    "print(\"最佳參數:\", random_search.best_params_)\n",
    "print(\"最佳F1-score:\", random_search.best_score_)\n",
    "\n",
    "# === 📊 交叉驗證評估 ===\n",
    "print(\"📊 執行交叉驗證...\")\n",
    "best_model = random_search.best_estimator_\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "f1_scores = cross_val_score(best_model, X_train[top_features], y_train, cv=cv, scoring='f1')\n",
    "print(f\"交叉驗證 F1-score 平均值: {f1_scores.mean():.4f}\")\n",
    "\n",
    "# === 🧠 找最佳 Threshold ===\n",
    "print(\"🧠 搜尋最佳 Threshold 中（precision/recall/F1）...\")\n",
    "y_probs = best_model.predict_proba(X_test[top_features])[:, 1]\n",
    "thresholds = np.arange(0.1, 0.91, 0.01)\n",
    "best_f1 = 0\n",
    "best_threshold = 0.4\n",
    "f1_list, prec_list, recall_list = [], [], []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    preds = (y_probs >= threshold).astype(int)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, preds, average='binary')\n",
    "    f1_list.append(f1)\n",
    "    prec_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"\\n🎯 最佳 threshold: {best_threshold:.2f}\")\n",
    "print(f\"🔹 對應 F1-score: {best_f1:.4f}\")\n",
    "\n",
    "# === 📈 繪圖：Threshold vs Precision / Recall / F1 ===\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, prec_list, label='Precision')\n",
    "plt.plot(thresholds, recall_list, label='Recall')\n",
    "plt.plot(thresholds, f1_list, label='F1-score')\n",
    "plt.axvline(best_threshold, color='r', linestyle='--', label=f'Best threshold = {best_threshold:.2f}')\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Threshold vs Precision / Recall / F1\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === 📤 測試資料預測，使用最佳 threshold ===\n",
    "for col in top_features:\n",
    "    if col not in test_df.columns:\n",
    "        test_df[col] = np.nan\n",
    "X_public = test_df[top_features].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "test_probs = best_model.predict_proba(X_public)[:, 1]\n",
    "test_df[\"飆股\"] = (test_probs >= best_threshold).astype(int)\n",
    "\n",
    "# === 💾 輸出 submission 結果 ===\n",
    "submission = test_df[[\"ID\", \"飆股\"]]\n",
    "submission.to_csv(\"public_result1.csv\", index=False, encoding=\"utf-8\", lineterminator='\\n')\n",
    "print(\"✅ 成功輸出 submission：public_result1.csv with optimal threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a0c0997-57fe-4988-8fff-16ea585737d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 資料預處理開始...\n",
      "🪛 Cleaning training data...\n",
      "🪛 Cleaning testing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing feature groups: 100%|█████████████████████████████████████████████████████████| 9/9 [00:02<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 處理完成後的特徵數量：10244 個\n",
      "✅ 資料處理完成，耗時：102.53 秒\n"
     ]
    }
   ],
   "source": [
    "#LSTM未做神經網路訓練\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"\\U0001F4E6 資料預處理開始...\")\n",
    "\n",
    "# === 🪜 處理缺失值與資料型態（推薦寫法）===\n",
    "print(\"🪛 Cleaning training data...\")\n",
    "float64_cols_train = train_df.select_dtypes(include='float64').columns\n",
    "train_df[float64_cols_train] = train_df[float64_cols_train].astype('float32').replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "print(\"🪛 Cleaning testing data...\")\n",
    "float64_cols_test = test_df.select_dtypes(include='float64').columns\n",
    "test_df[float64_cols_test] = test_df[float64_cols_test].astype('float32').replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# === 🌟 特定特徵群處理 ===\n",
    "feature_groups = [\n",
    "    \"官股券商_\", \"個股券商分點\", \"個股主力買賣超統計\",\n",
    "    \"日外資_\", \"日自營_\", \"日投信_\",\n",
    "    \"技術指標_\", \"月營收_\", \"季IFRS財報_\"\n",
    "]\n",
    "\n",
    "for prefix in tqdm(feature_groups, desc=\"Processing feature groups\"):\n",
    "    cols = [col for col in train_df.columns if col.startswith(prefix)]\n",
    "    train_df[cols] = train_df[cols].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# === 📈 使用 LSTM 處理時間序列特徵 ===\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=16):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, (h_n, _) = self.lstm(x)\n",
    "        return h_n[-1]\n",
    "\n",
    "# === LSTM 1: 個股價格/量/大盤 ===\n",
    "seq_cols_1 = [\n",
    "    [f\"個股前{i}天收盤價\" for i in range(1, 21)],\n",
    "    [f\"個股前{i}天成交量\" for i in range(1, 21)],\n",
    "    [f\"上市加權指數前{i}天收盤價\" for i in range(1, 21)],\n",
    "    [f\"上市加權指數前{i}天成交量\" for i in range(1, 21)]\n",
    "]\n",
    "all_seq_cols_1 = sum(seq_cols_1, [])\n",
    "scaler_1 = StandardScaler()\n",
    "\n",
    "X_seq_1 = scaler_1.fit_transform(train_df[all_seq_cols_1])\n",
    "X_seq_1 = X_seq_1.reshape(len(train_df), 20, -1)\n",
    "X_seq_tensor_1 = torch.tensor(X_seq_1, dtype=torch.float32)\n",
    "model_1 = SimpleLSTM(input_size=X_seq_1.shape[2])\n",
    "with torch.no_grad():\n",
    "    lstm_output_1 = model_1(X_seq_tensor_1).numpy()\n",
    "lstm_cols_1 = [f'LSTM_seq1_embed_{i}' for i in range(lstm_output_1.shape[1])]\n",
    "train_df.drop(columns=[col for col in lstm_cols_1 if col in train_df.columns], inplace=True)\n",
    "lstm_df1 = pd.DataFrame(lstm_output_1, columns=lstm_cols_1)\n",
    "train_df = pd.concat([train_df, lstm_df1], axis=1)\n",
    "\n",
    "# 測試資料同步處理\n",
    "X_seq_1_test = scaler_1.transform(test_df[all_seq_cols_1])\n",
    "X_seq_1_test = X_seq_1_test.reshape(len(test_df), 20, -1)\n",
    "X_seq_tensor_1_test = torch.tensor(X_seq_1_test, dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    lstm_output_1_test = model_1(X_seq_tensor_1_test).numpy()\n",
    "test_df.drop(columns=[col for col in lstm_cols_1 if col in test_df.columns], inplace=True)\n",
    "lstm_df1_test = pd.DataFrame(lstm_output_1_test, columns=lstm_cols_1)\n",
    "test_df = pd.concat([test_df, lstm_df1_test], axis=1)\n",
    "\n",
    "# === LSTM 2: 主力券商資料 ===\n",
    "seq_cols_2 = [\n",
    "    col for col in train_df.columns if any(\n",
    "        col.startswith(f\"{side}超第{rank}名分點前\") and not col.endswith(\"券商代號\")\n",
    "        for side in [\"買\", \"賣\"] for rank in range(1, 16)\n",
    "    )\n",
    "]\n",
    "scaler_2 = StandardScaler()\n",
    "X_seq_2 = scaler_2.fit_transform(train_df[seq_cols_2])\n",
    "X_seq_2 = X_seq_2.reshape(len(train_df), 20, -1)\n",
    "X_seq_tensor_2 = torch.tensor(X_seq_2, dtype=torch.float32)\n",
    "model_2 = SimpleLSTM(input_size=X_seq_2.shape[2])\n",
    "with torch.no_grad():\n",
    "    lstm_output_2 = model_2(X_seq_tensor_2).numpy()\n",
    "lstm_cols_2 = [f'LSTM_seq2_embed_{i}' for i in range(lstm_output_2.shape[1])]\n",
    "train_df.drop(columns=[col for col in lstm_cols_2 if col in train_df.columns], inplace=True)\n",
    "lstm_df2 = pd.DataFrame(lstm_output_2, columns=lstm_cols_2)\n",
    "train_df = pd.concat([train_df, lstm_df2], axis=1)\n",
    "\n",
    "# 測試資料同步處理\n",
    "X_seq_2_test = scaler_2.transform(test_df[seq_cols_2])\n",
    "X_seq_2_test = X_seq_2_test.reshape(len(test_df), 20, -1)\n",
    "X_seq_tensor_2_test = torch.tensor(X_seq_2_test, dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    lstm_output_2_test = model_2(X_seq_tensor_2_test).numpy()\n",
    "test_df.drop(columns=[col for col in lstm_cols_2 if col in test_df.columns], inplace=True)\n",
    "lstm_df2_test = pd.DataFrame(lstm_output_2_test, columns=lstm_cols_2)\n",
    "test_df = pd.concat([test_df, lstm_df2_test], axis=1)\n",
    "\n",
    "# === 🌿 特徵與標籤分離 ===\n",
    "target = \"飆股\"\n",
    "features = [col for col in train_df.columns if col not in [\"ID\", target]]\n",
    "X = train_df[features]\n",
    "y = train_df[target]\n",
    "\n",
    "print(f\"📊 處理完成後的特徵數量：{len(features)} 個\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"✅ 資料處理完成，耗時：{end_time - start_time:.2f} 秒\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d96f66e2-c4f6-4b4a-8c39-f16790cdf353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 資料預處理開始...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning training data: 100%|██████████████████████████████████████████████████| 12736/12736 [00:11<00:00, 1068.92it/s]\n",
      "Processing feature groups: 100%|█████████████████████████████████████████████████████████| 9/9 [00:02<00:00,  3.95it/s]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 9.53 GiB for an array with shape (12734, 200864) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20460\\300001421.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# === ✅ 建立特徵儲存框架 ===\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mtrain_features_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# === 📊 MA 特徵工程 ===\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m   6809\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6810\u001b[0m         \"\"\"\n\u001b[1;32m-> 6811\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6812\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6813\u001b[0m         return self._constructor_from_mgr(data, axes=data.axes).__finalize__(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 604\u001b[1;33m             \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    605\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1786\u001b[0m         \u001b[1;31m#  BlockManager objects not yet attached to a DataFrame.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1787\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_consolidated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1788\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1789\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1790\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_known_consolidated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   2267\u001b[0m     \u001b[0mnew_blocks\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mBlock\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2268\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_blocks\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrouper\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2269\u001b[1;33m         merged_blocks, _ = _merge_blocks(\n\u001b[0m\u001b[0;32m   2270\u001b[0m             \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcan_consolidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2271\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[0;32m   2292\u001b[0m             \u001b[1;31m# Sequence[Union[int, float, complex, str, bytes, generic]],\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2293\u001b[0m             \u001b[1;31m# Sequence[Sequence[Any]], SupportsArray]]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2294\u001b[1;33m             \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2295\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2296\u001b[0m             \u001b[0mbvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mblk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[0marrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcasting\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 9.53 GiB for an array with shape (12734, 200864) and data type float32"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"📦 資料預處理開始...\")\n",
    "\n",
    "# === 🧼 缺失值與型態處理 ===\n",
    "for col in tqdm(train_df.columns, desc=\"Cleaning training data\"):\n",
    "    if train_df[col].dtype == 'float64':\n",
    "        train_df[col] = train_df[col].astype('float32').replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# === 🎯 特定特徵群補值處理 ===\n",
    "feature_groups = [\n",
    "    \"官股券商_\", \"個股券商分點\", \"個股主力買賣超統計\",\n",
    "    \"日外資_\", \"日自營_\", \"日投信_\",\n",
    "    \"技術指標_\", \"月營收_\", \"季IFRS財報_\"\n",
    "]\n",
    "for prefix in tqdm(feature_groups, desc=\"Processing feature groups\"):\n",
    "    cols = [col for col in train_df.columns if col.startswith(prefix)]\n",
    "    train_df[cols] = train_df[cols].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# === ✅ 建立特徵儲存框架 ===\n",
    "train_features_df = train_df.copy()\n",
    "\n",
    "# === 📊 MA 特徵工程 ===\n",
    "types = [\n",
    "    \"張增減\", \"金額增減(千)\", \"買張\", \"賣張\", \"買金額(千)\", \"賣金額(千)\",\n",
    "    \"買筆數\", \"賣筆數\", \"買均張\", \"賣均張\", \"買均價\", \"賣均價\", \"買均值(千)\", \"賣均值(千)\"\n",
    "]\n",
    "\n",
    "print(\"📊 生成 MA 特徵\")\n",
    "for rank in tqdm(range(1, 16), desc=\"MA 特徵\"):\n",
    "    for t in types:\n",
    "        for ma in [5, 10, 20]:\n",
    "            buy_cols = [f\"買超第{rank}名分點前{i}天{t}\" for i in range(1, ma + 1)]\n",
    "            sell_cols = [f\"賣超第{rank}名分點前{i}天{t}\" for i in range(1, ma + 1)]\n",
    "\n",
    "            if all(col in train_df.columns for col in buy_cols):\n",
    "                train_features_df[f\"買超第{rank}名_{t}_MA{ma}\"] = train_df[buy_cols].mean(axis=1)\n",
    "\n",
    "            if all(col in train_df.columns for col in sell_cols):\n",
    "                train_features_df[f\"賣超第{rank}名_{t}_MA{ma}\"] = train_df[sell_cols].mean(axis=1)\n",
    "\n",
    "# === 📊 EMA 特徵工程 ===\n",
    "print(\"📊 生成 EMA 特徵\")\n",
    "for rank in tqdm(range(1, 16), desc=\"EMA 特徵\"):\n",
    "    for t in types:\n",
    "        for span in [5, 10, 20]:\n",
    "            buy_cols = [f\"買超第{rank}名分點前{i}天{t}\" for i in range(1, span + 1)]\n",
    "            sell_cols = [f\"賣超第{rank}名分點前{i}天{t}\" for i in range(1, span + 1)]\n",
    "\n",
    "            if all(col in train_df.columns for col in buy_cols):\n",
    "                reversed_buy = np.fliplr(train_df[buy_cols].values)\n",
    "                ema_vals = pd.DataFrame(reversed_buy).ewm(span=span, adjust=False).mean().iloc[:, -1].values\n",
    "                train_features_df[f\"買超第{rank}名_{t}_EMA{span}\"] = ema_vals\n",
    "\n",
    "            if all(col in train_df.columns for col in sell_cols):\n",
    "                reversed_sell = np.fliplr(train_df[sell_cols].values)\n",
    "                ema_vals = pd.DataFrame(reversed_sell).ewm(span=span, adjust=False).mean().iloc[:, -1].values\n",
    "                train_features_df[f\"賣超第{rank}名_{t}_EMA{span}\"] = ema_vals\n",
    "\n",
    "# === 📊 技術指標 ===\n",
    "train_features_df['RSI_diff'] = train_df['技術指標_RSI(10)'].diff().fillna(0)\n",
    "train_features_df['乖離率_change'] = train_df['技術指標_乖離率(20日)'].pct_change().fillna(0)\n",
    "\n",
    "# === 📊 個股報酬率與波動度 ===\n",
    "close_cols = [f'個股前{i}天收盤價' for i in range(1, 21)]\n",
    "train_features_df['個股1天報酬率'] = (train_df['個股收盤價'] - train_df['個股前1天收盤價']) / train_df['個股前1天收盤價']\n",
    "train_features_df['個股5天報酬率'] = (train_df['個股收盤價'] - train_df['個股前5天收盤價']) / train_df['個股前5天收盤價']\n",
    "train_features_df['個股10天報酬率'] = (train_df['個股收盤價'] - train_df['個股前10天收盤價']) / train_df['個股前10天收盤價']\n",
    "train_features_df['個股20天報酬率'] = (train_df['個股收盤價'] - train_df['個股前20天收盤價']) / train_df['個股前20天收盤價']\n",
    "train_features_df['個股5天波動度'] = train_df[close_cols[:5]].std(axis=1)\n",
    "train_features_df['個股10天波動度'] = train_df[close_cols[:10]].std(axis=1)\n",
    "train_features_df['個股20天波動度'] = train_df[close_cols].std(axis=1)\n",
    "train_features_df['個股5天乖離率'] = (train_df['個股收盤價'] - train_df[close_cols[:5]].mean(axis=1)) / train_df[close_cols[:5]].mean(axis=1)\n",
    "train_features_df['個股10天乖離率'] = (train_df['個股收盤價'] - train_df[close_cols[:10]].mean(axis=1)) / train_df[close_cols[:10]].mean(axis=1)\n",
    "train_features_df['個股19天乖離率'] = (train_df['個股收盤價'] - train_df[close_cols[:19]].mean(axis=1)) / train_df[close_cols[:19]].mean(axis=1)\n",
    "\n",
    "# === 📊 成交量波動度 ===\n",
    "volume_cols = [f'個股前{i}天成交量' for i in range(1, 21)]\n",
    "train_features_df['個股5天成交量波動度'] = train_df[volume_cols[:5]].std(axis=1)\n",
    "train_features_df['個股10天成交量波動度'] = train_df[volume_cols[:10]].std(axis=1)\n",
    "train_features_df['個股20天成交量波動度'] = train_df[volume_cols].std(axis=1)\n",
    "\n",
    "# === 📊 上市加權指數特徵 ===\n",
    "market_close_cols = [f'上市加權指數前{i}天收盤價' for i in range(1, 21)]\n",
    "market_vol_cols = [f'上市加權指數前{i}天成交量' for i in range(1, 21)]\n",
    "train_features_df['上市加權指數1天報酬率'] = (train_df['上市加權指數收盤價'] - train_df['上市加權指數前1天收盤價']) / train_df['上市加權指數前1天收盤價']\n",
    "train_features_df['上市加權指數5天報酬率'] = (train_df['上市加權指數收盤價'] - train_df['上市加權指數前5天收盤價']) / train_df['上市加權指數前5天收盤價']\n",
    "train_features_df['上市加權指數10天報酬率'] = (train_df['上市加權指數收盤價'] - train_df['上市加權指數前10天收盤價']) / train_df['上市加權指數前10天收盤價']\n",
    "train_features_df['上市加權指數20天報酬率'] = (train_df['上市加權指數收盤價'] - train_df['上市加權指數前20天收盤價']) / train_df['上市加權指數前20天收盤價']\n",
    "train_features_df['上市加權指數5天波動度'] = train_df[market_close_cols[:5]].std(axis=1)\n",
    "train_features_df['上市加權指數10天波動度'] = train_df[market_close_cols[:10]].std(axis=1)\n",
    "train_features_df['上市加權指數20天波動度'] = train_df[market_close_cols].std(axis=1)\n",
    "train_features_df['上市加權指數5天乖離率'] = (train_df['上市加權指數收盤價'] - train_df[market_close_cols[:5]].mean(axis=1)) / train_df[market_close_cols[:5]].mean(axis=1)\n",
    "train_features_df['上市加權指數10天乖離率'] = (train_df['上市加權指數收盤價'] - train_df[market_close_cols[:10]].mean(axis=1)) / train_df[market_close_cols[:10]].mean(axis=1)\n",
    "train_features_df['上市加權指數19天乖離率'] = (train_df['上市加權指數收盤價'] - train_df[market_close_cols[:19]].mean(axis=1)) / train_df[market_close_cols[:19]].mean(axis=1)\n",
    "train_features_df['上市加權指數5天成交量波動度'] = train_df[market_vol_cols[:5]].std(axis=1)\n",
    "train_features_df['上市加權指數10天成交量波動度'] = train_df[market_vol_cols[:10]].std(axis=1)\n",
    "train_features_df['上市加權指數20天成交量波動度'] = train_df[market_vol_cols].std(axis=1)\n",
    "\n",
    "# === 🎯 特徵與標籤分離 ===\n",
    "target = \"飆股\"\n",
    "features = [col for col in train_features_df.columns if col not in [\"ID\", target]]\n",
    "X = train_features_df[features]\n",
    "y = train_features_df[target]\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"✅ 資料處理完成，耗時：{end_time - start_time:.2f} 秒\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e262c405-28bb-4a09-9558-30fadd42a9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89cde44a-5240-47a4-a80d-218e1373b378",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning training data: 100%|█████████████████████████████████████████████████| 11476/11476 [00:00<00:00, 19190.20it/s]\n",
      "Cleaning testing data: 100%|█████████████████████████████████████████████████| 10214/10214 [00:00<00:00, 292178.15it/s]\n",
      "C:\\Users\\james\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\training.py:183: UserWarning: [16:55:17] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 40 candidates, totalling 120 fits\n",
      "最佳參數: {'subsample': 0.9, 'reg_lambda': 10, 'reg_alpha': 0.1, 'n_estimators': 800, 'max_depth': 8, 'learning_rate': 0.1, 'gamma': 0, 'colsample_bytree': 0.9}\n",
      "最佳F1-score: 0.7048661935785835\n",
      "交叉驗證 F1-score 平均值: 0.7367\n",
      "✅ 成功輸出 submission：public_result1.csv\n"
     ]
    }
   ],
   "source": [
    "#初始版本(已寫入特徵工程)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import shap\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# === 🧪 切分資料 ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_df[features], y, stratify=y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# === 🔧 設定類別權重 ===\n",
    "scale_pos_weight = y_train.value_counts()[0] / y_train.value_counts()[1]\n",
    "\n",
    "# === 📈 模型直接訓練 (不使用SMOTE或ROS) ===\n",
    "model = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    gamma=1,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# === 📊 SHAP重要特徵篩選（使用更多資料） ===\n",
    "X_sample = X_train.sample(n=2000, random_state=42)\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "shap_importance = pd.DataFrame({\n",
    "    'feature': X_sample.columns,\n",
    "    'mean_abs_shap': np.abs(shap_values).mean(axis=0)\n",
    "}).sort_values(by='mean_abs_shap', ascending=False)\n",
    "\n",
    "top_features = shap_importance.head(50)['feature'].tolist()\n",
    "\n",
    "# === ♻️ 使用top特徵與GridSearchCV進行調參 ===\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [600, 800, 1000],\n",
    "    'max_depth': [6, 8, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "    'gamma': [0, 1],\n",
    "    'reg_alpha': [0, 0.1],\n",
    "    'reg_lambda': [1, 5, 10]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=XGBClassifier(\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        eval_metric='logloss',\n",
    "        random_state=42\n",
    "    ),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=100,  # 搜尋40組參數\n",
    "    scoring='f1',\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(X_train[top_features], y_train)\n",
    "print(\"最佳參數:\", random_search.best_params_)\n",
    "print(\"最佳F1-score:\", random_search.best_score_)\n",
    "\n",
    "\n",
    "# === 📈 最佳模型交叉驗證評估 ===\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "f1_scores = cross_val_score(best_model, X_train[top_features], y_train, cv=cv, scoring='f1')\n",
    "print(f\"交叉驗證 F1-score 平均值: {f1_scores.mean():.4f}\")\n",
    "\n",
    "# === 📤 測試資料預測 ===\n",
    "X_public = test_df[top_features].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "test_df[\"飆股\"] = best_model.predict(X_public)\n",
    "\n",
    "# === 💾 輸出 submission 結果 ===\n",
    "submission = test_df[[\"ID\", \"飆股\"]]\n",
    "submission.to_csv(\"public_result1.csv\", index=False, encoding=\"utf-8\", lineterminator='\\n')\n",
    "print(\"✅ 成功輸出 submission：public_result1.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ea32ea3-17de-42ab-aecd-ab7e6e5d8b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 🎯 官股券商 & 籌碼分析特徵工程 ===\n",
    "\n",
    "# 官股券商欄位\n",
    "gov_cols = [col for col in train_df.columns if \"官股券商_\" in col]\n",
    "train_df[gov_cols] = train_df[gov_cols].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# 個股籌碼分析欄位（含熱門度、買賣分布係數、分點密度）\n",
    "chip_cols = [col for col in train_df.columns if \"個股券商分點\" in col]\n",
    "train_df[chip_cols] = train_df[chip_cols].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# 主力買賣超統計資料\n",
    "main_force_cols = [col for col in train_df.columns if \"個股主力買賣超統計\" in col]\n",
    "train_df[main_force_cols] = train_df[main_force_cols].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# 外資、自營、投信買賣欄位\n",
    "inst_cols = [col for col in train_df.columns if col.startswith(\"日外資_\") or col.startswith(\"日自營_\") or col.startswith(\"日投信_\")]\n",
    "train_df[inst_cols] = train_df[inst_cols].replace([np.inf, -np.inf], np.nan).fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "061e79fc-e25b-48ba-a6b1-a9f87151128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 🎯 技術指標特徵處理 ===\n",
    "\n",
    "# 抓出所有技術指標欄位\n",
    "tech_cols = [col for col in train_df.columns if col.startswith(\"技術指標_\")]\n",
    "\n",
    "# 統一處理缺失與無限值\n",
    "train_df[tech_cols] = train_df[tech_cols].replace([np.inf, -np.inf], np.nan).fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c6aae3b-4a2e-4ee3-801c-d883f29d0242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 🎯 基本面（月營收）特徵處理 ===\n",
    "\n",
    "# 所有以「月營收_」開頭的欄位統一抓取\n",
    "fundamental_cols = [col for col in train_df.columns if col.startswith(\"月營收_\")]\n",
    "\n",
    "# 缺失處理與無限值補 0（某些財報欄位偶爾會空或除以 0）\n",
    "train_df[fundamental_cols] = train_df[fundamental_cols].replace([np.inf, -np.inf], np.nan).fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7776d91-4186-41f8-b807-7e6ac31a672c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 📊 季IFRS財報特徵處理 ===\n",
    "\n",
    "# 擷取所有季IFRS財報欄位\n",
    "ifrs_cols = [col for col in train_df.columns if col.startswith(\"季IFRS財報_\")]\n",
    "\n",
    "# 替換 inf 與 NaN 為 0（避免報表計算錯誤或除以零）\n",
    "train_df[ifrs_cols] = train_df[ifrs_cols].replace([np.inf, -np.inf], np.nan).fillna(0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "dl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
