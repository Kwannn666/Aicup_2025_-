{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e916a3c-5261-4d3c-a8bc-cc4f8fefb432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¹ æ­£åœ¨æ¸…é™¤è®€å…¥çš„è³‡æ–™ DataFrame...\n",
      "âœ… æ¸…é™¤å®Œæˆï¼Œå¯é‡æ–°åŸ·è¡Œè³‡æ–™è¼‰å…¥ç¨‹å¼\n"
     ]
    }
   ],
   "source": [
    "# === ğŸ§¹ æ¸…é™¤ train_df / test_df ç­‰ç›¸é—œè®Šæ•¸èˆ‡é‡‹æ”¾è¨˜æ†¶é«” ===\n",
    "import gc\n",
    "\n",
    "print(\"ğŸ§¹ æ­£åœ¨æ¸…é™¤è®€å…¥çš„è³‡æ–™ DataFrame...\")\n",
    "\n",
    "# ç¢ºä¿è®Šæ•¸å­˜åœ¨æ‰åˆªé™¤ï¼Œé¿å…å ±éŒ¯\n",
    "if 'train_df' in locals():\n",
    "    del train_df\n",
    "if 'test_df' in locals():\n",
    "    del test_df\n",
    "\n",
    "# å˜—è©¦é‡‹æ”¾è¨˜æ†¶é«”\n",
    "gc.collect()\n",
    "\n",
    "print(\"âœ… æ¸…é™¤å®Œæˆï¼Œå¯é‡æ–°åŸ·è¡Œè³‡æ–™è¼‰å…¥ç¨‹å¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "841c861d-d013-4b9d-9c58-3bf383fc2aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ æ­£åœ¨è®€å– training.csv ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading training.csv: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:01<00:00, 65.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ æ­£åœ¨è®€å– public_x.csv ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading public_x.csv: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:01<00:00, 64.61it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# === Step 1: æ¨¡æ“¬è®€å–è³‡æ–™é€²åº¦æ¢ ===\n",
    "print(\"ğŸ“‚ æ­£åœ¨è®€å– training.csv ...\")\n",
    "for _ in tqdm(range(100), desc=\"Loading training.csv\"):\n",
    "    time.sleep(0.002)\n",
    "\n",
    "train_df = pd.read_csv(\"training.csv\")\n",
    "\n",
    "print(\"ğŸ“‚ æ­£åœ¨è®€å– public_x.csv ...\")\n",
    "for _ in tqdm(range(100), desc=\"Loading public_x.csv\"):\n",
    "    time.sleep(0.002)\n",
    "\n",
    "test_df = pd.read_csv(\"public_x.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5addfc1f-c727-4958-bb67-eebf6668acbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¹ æ¸…é™¤ä¹‹å‰æ¨¡å‹çš„è¨“ç·´çµæœå’Œè®Šæ•¸...\n",
      "ğŸ”„ é‡ç½®å®Œæˆï¼Œå¯ä»¥é‡æ–°é€²è¡Œæ¨¡å‹è¨“ç·´\n"
     ]
    }
   ],
   "source": [
    "# === ğŸ§¹ æ¸…é™¤è¨“ç·´çµæœä¸¦é‡è¨­ç’°å¢ƒï¼ˆä¿ç•™è³‡æ–™ï¼‰ ===\n",
    "import gc\n",
    "\n",
    "print(\"ğŸ§¹ æ¸…é™¤ä¹‹å‰æ¨¡å‹çš„è¨“ç·´çµæœå’Œè®Šæ•¸...\")\n",
    "for var in ['X', 'y', 'X_train', 'X_test', 'y_train', 'y_test', 'X_train', 'y_train',\n",
    "            'model', 'best_model', 'grid_search', 'shap_values', 'shap_importance']:\n",
    "    if var in globals():\n",
    "        del globals()[var]\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print(\"ğŸ”„ é‡ç½®å®Œæˆï¼Œå¯ä»¥é‡æ–°é€²è¡Œæ¨¡å‹è¨“ç·´\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22bccef4-4d7c-4b11-b0c6-a00ccce88182",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11756\\1836817306.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# === ğŸ“¥ è¼‰å…¥è³‡æ–™ ===\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, RandomizedSearchCV\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, f1_score\n",
    "import shap\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['font.family'] = 'Microsoft JhengHei'  # æ”¯æ´ä¸­æ–‡å­—å‹\n",
    "\n",
    "# === ğŸ“¥ è¼‰å…¥è³‡æ–™ ===\n",
    "X = train_df[features].replace([np.inf, -np.inf], np.nan).fillna(0).astype(np.float32)\n",
    "y = y.astype(int)\n",
    "\n",
    "# === âœ‚ï¸ åˆ‡åˆ†è¨“ç·´ / æ¸¬è©¦è³‡æ–™ ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# === âš–ï¸ è¨ˆç®—é¡åˆ¥æ¬Šé‡ ===\n",
    "print(\"âš–ï¸ è¨ˆç®—é¡åˆ¥æ¬Šé‡...\")\n",
    "scale_pos_weight = y_train.value_counts()[0] / y_train.value_counts()[1]\n",
    "\n",
    "# === ğŸ“ˆ åˆå§‹ LightGBM æ¨¡å‹è¨“ç·´ï¼ˆfor SHAPï¼‰ ===\n",
    "print(\"ğŸ“ˆ è¨“ç·´åˆå§‹ LightGBM æ¨¡å‹ï¼ˆfor SHAP åˆ†æï¼‰...\")\n",
    "model = LGBMClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=10,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    device='gpu',\n",
    "    boosting_type='gbdt',\n",
    "    objective='binary',\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# === ğŸ” è¨ˆç®— SHAP å€¼ ===\n",
    "print(\"ğŸ” è¨ˆç®— SHAP å€¼ä»¥ç¯©é¸é‡è¦ç‰¹å¾µ...\")\n",
    "X_sample = X_train.sample(n=8000, random_state=42)\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "# å®‰å…¨æª¢æŸ¥ shap_values æ˜¯å¦ç‚º listï¼ˆæ–°ç‰ˆ LightGBM æœ‰å¯èƒ½æ˜¯ listï¼Œä¹Ÿå¯èƒ½æ˜¯ arrayï¼‰\n",
    "if isinstance(shap_values, list):\n",
    "    shap_vals = shap_values[1]  # æ­£é¡\n",
    "else:\n",
    "    shap_vals = shap_values     # è‹¥ä¸æ˜¯ list å°±ç›´æ¥ä½¿ç”¨\n",
    "\n",
    "shap_importance = pd.DataFrame({\n",
    "    'feature': X_sample.columns,\n",
    "    'mean_abs_shap': np.abs(shap_vals).mean(axis=0)\n",
    "}).sort_values(by='mean_abs_shap', ascending=False)\n",
    "\n",
    "# ç¹ªåœ–\n",
    "shap.summary_plot(shap_vals, X_sample, max_display=30)\n",
    "\n",
    "# åŒ¯å‡ºé‡è¦ç‰¹å¾µ\n",
    "top_features = shap_importance.head(500)['feature'].tolist()\n",
    "shap_importance.head(100).to_csv(\"shap_top_features.csv\", index=False)\n",
    "\n",
    "# === ğŸ”§ RandomizedSearchCV èª¿åƒ ===\n",
    "print(\"ğŸ¯ é–‹å§‹ RandomizedSearchCV èª¿åƒ...\")\n",
    "param_dist = {\n",
    "    'n_estimators': [600, 800, 1000, 1200],\n",
    "    'max_depth': [8, 10, 12, 20],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "    'reg_alpha': [0, 0.1],\n",
    "    'reg_lambda': [1, 5, 10],\n",
    "    'min_data_in_leaf': [10, 20, 50],\n",
    "    'min_gain_to_split': [0, 0.001, 0.01],\n",
    "    'max_bin': [255],\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=LGBMClassifier(\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        objective='binary',\n",
    "        device='gpu',\n",
    "        boosting_type='gbdt',\n",
    "        random_state=42\n",
    "    ),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,\n",
    "    scoring='f1',\n",
    "    cv=3,\n",
    "    verbose=3,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "random_search.fit(X_train[top_features], y_train)\n",
    "\n",
    "print(\"âœ… èª¿åƒå®Œæˆï¼\")\n",
    "print(\"æœ€ä½³åƒæ•¸:\", random_search.best_params_)\n",
    "print(\"æœ€ä½³F1-score:\", random_search.best_score_)\n",
    "\n",
    "# === ğŸ“Š äº¤å‰é©—è­‰è©•ä¼° ===\n",
    "print(\"ğŸ“Š åŸ·è¡Œäº¤å‰é©—è­‰...\")\n",
    "best_model = random_search.best_estimator_\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "f1_scores = cross_val_score(best_model, X_train[top_features], y_train, cv=cv, scoring='f1')\n",
    "print(f\"äº¤å‰é©—è­‰ F1-score å¹³å‡å€¼: {f1_scores.mean():.4f}\")\n",
    "\n",
    "# === ğŸ§  æ‰¾æœ€ä½³ Threshold ===\n",
    "print(\"ğŸ§  æœå°‹æœ€ä½³ Threshold ä¸­ï¼ˆprecision/recall/F1ï¼‰...\")\n",
    "y_probs = best_model.predict_proba(X_test[top_features])[:, 1]\n",
    "thresholds = np.arange(0.1, 0.91, 0.01)\n",
    "best_f1 = 0\n",
    "best_threshold = 0.4\n",
    "f1_list, prec_list, recall_list = [], [], []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    preds = (y_probs >= threshold).astype(int)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, preds, average='binary')\n",
    "    f1_list.append(f1)\n",
    "    prec_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"\\nğŸ¯ æœ€ä½³ threshold: {best_threshold:.2f}\")\n",
    "print(f\"ğŸ”¹ å°æ‡‰ F1-score: {best_f1:.4f}\")\n",
    "\n",
    "# === ğŸ“ˆ ç¹ªåœ–ï¼šThreshold vs Precision / Recall / F1 ===\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, prec_list, label='Precision')\n",
    "plt.plot(thresholds, recall_list, label='Recall')\n",
    "plt.plot(thresholds, f1_list, label='F1-score')\n",
    "plt.axvline(best_threshold, color='r', linestyle='--', label=f'Best threshold = {best_threshold:.2f}')\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Threshold vs Precision / Recall / F1\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === ğŸ“¤ æ¸¬è©¦è³‡æ–™é æ¸¬ï¼Œä½¿ç”¨æœ€ä½³ threshold ===\n",
    "for col in top_features:\n",
    "    if col not in test_df.columns:\n",
    "        test_df[col] = np.nan\n",
    "X_public = test_df[top_features].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "test_probs = best_model.predict_proba(X_public)[:, 1]\n",
    "test_df[\"é£†è‚¡\"] = (test_probs >= best_threshold).astype(int)\n",
    "\n",
    "# === ğŸ’¾ è¼¸å‡º submission çµæœ ===\n",
    "submission = test_df[[\"ID\", \"é£†è‚¡\"]]\n",
    "submission.to_csv(\"public_result1.csv\", index=False, encoding=\"utf-8\", lineterminator='\\n')\n",
    "print(\"âœ… æˆåŠŸè¼¸å‡º submissionï¼špublic_result1.csv with optimal threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a0c0997-57fe-4988-8fff-16ea585737d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ è³‡æ–™é è™•ç†é–‹å§‹...\n",
      "ğŸª› Cleaning training data...\n",
      "ğŸª› Cleaning testing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing feature groups: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:02<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š è™•ç†å®Œæˆå¾Œçš„ç‰¹å¾µæ•¸é‡ï¼š10244 å€‹\n",
      "âœ… è³‡æ–™è™•ç†å®Œæˆï¼Œè€—æ™‚ï¼š102.53 ç§’\n"
     ]
    }
   ],
   "source": [
    "#LSTMæœªåšç¥ç¶“ç¶²è·¯è¨“ç·´\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"\\U0001F4E6 è³‡æ–™é è™•ç†é–‹å§‹...\")\n",
    "\n",
    "# === ğŸªœ è™•ç†ç¼ºå¤±å€¼èˆ‡è³‡æ–™å‹æ…‹ï¼ˆæ¨è–¦å¯«æ³•ï¼‰===\n",
    "print(\"ğŸª› Cleaning training data...\")\n",
    "float64_cols_train = train_df.select_dtypes(include='float64').columns\n",
    "train_df[float64_cols_train] = train_df[float64_cols_train].astype('float32').replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "print(\"ğŸª› Cleaning testing data...\")\n",
    "float64_cols_test = test_df.select_dtypes(include='float64').columns\n",
    "test_df[float64_cols_test] = test_df[float64_cols_test].astype('float32').replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# === ğŸŒŸ ç‰¹å®šç‰¹å¾µç¾¤è™•ç† ===\n",
    "feature_groups = [\n",
    "    \"å®˜è‚¡åˆ¸å•†_\", \"å€‹è‚¡åˆ¸å•†åˆ†é»\", \"å€‹è‚¡ä¸»åŠ›è²·è³£è¶…çµ±è¨ˆ\",\n",
    "    \"æ—¥å¤–è³‡_\", \"æ—¥è‡ªç‡Ÿ_\", \"æ—¥æŠ•ä¿¡_\",\n",
    "    \"æŠ€è¡“æŒ‡æ¨™_\", \"æœˆç‡Ÿæ”¶_\", \"å­£IFRSè²¡å ±_\"\n",
    "]\n",
    "\n",
    "for prefix in tqdm(feature_groups, desc=\"Processing feature groups\"):\n",
    "    cols = [col for col in train_df.columns if col.startswith(prefix)]\n",
    "    train_df[cols] = train_df[cols].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# === ğŸ“ˆ ä½¿ç”¨ LSTM è™•ç†æ™‚é–“åºåˆ—ç‰¹å¾µ ===\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=16):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, (h_n, _) = self.lstm(x)\n",
    "        return h_n[-1]\n",
    "\n",
    "# === LSTM 1: å€‹è‚¡åƒ¹æ ¼/é‡/å¤§ç›¤ ===\n",
    "seq_cols_1 = [\n",
    "    [f\"å€‹è‚¡å‰{i}å¤©æ”¶ç›¤åƒ¹\" for i in range(1, 21)],\n",
    "    [f\"å€‹è‚¡å‰{i}å¤©æˆäº¤é‡\" for i in range(1, 21)],\n",
    "    [f\"ä¸Šå¸‚åŠ æ¬ŠæŒ‡æ•¸å‰{i}å¤©æ”¶ç›¤åƒ¹\" for i in range(1, 21)],\n",
    "    [f\"ä¸Šå¸‚åŠ æ¬ŠæŒ‡æ•¸å‰{i}å¤©æˆäº¤é‡\" for i in range(1, 21)]\n",
    "]\n",
    "all_seq_cols_1 = sum(seq_cols_1, [])\n",
    "scaler_1 = StandardScaler()\n",
    "\n",
    "X_seq_1 = scaler_1.fit_transform(train_df[all_seq_cols_1])\n",
    "X_seq_1 = X_seq_1.reshape(len(train_df), 20, -1)\n",
    "X_seq_tensor_1 = torch.tensor(X_seq_1, dtype=torch.float32)\n",
    "model_1 = SimpleLSTM(input_size=X_seq_1.shape[2])\n",
    "with torch.no_grad():\n",
    "    lstm_output_1 = model_1(X_seq_tensor_1).numpy()\n",
    "lstm_cols_1 = [f'LSTM_seq1_embed_{i}' for i in range(lstm_output_1.shape[1])]\n",
    "train_df.drop(columns=[col for col in lstm_cols_1 if col in train_df.columns], inplace=True)\n",
    "lstm_df1 = pd.DataFrame(lstm_output_1, columns=lstm_cols_1)\n",
    "train_df = pd.concat([train_df, lstm_df1], axis=1)\n",
    "\n",
    "# æ¸¬è©¦è³‡æ–™åŒæ­¥è™•ç†\n",
    "X_seq_1_test = scaler_1.transform(test_df[all_seq_cols_1])\n",
    "X_seq_1_test = X_seq_1_test.reshape(len(test_df), 20, -1)\n",
    "X_seq_tensor_1_test = torch.tensor(X_seq_1_test, dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    lstm_output_1_test = model_1(X_seq_tensor_1_test).numpy()\n",
    "test_df.drop(columns=[col for col in lstm_cols_1 if col in test_df.columns], inplace=True)\n",
    "lstm_df1_test = pd.DataFrame(lstm_output_1_test, columns=lstm_cols_1)\n",
    "test_df = pd.concat([test_df, lstm_df1_test], axis=1)\n",
    "\n",
    "# === LSTM 2: ä¸»åŠ›åˆ¸å•†è³‡æ–™ ===\n",
    "seq_cols_2 = [\n",
    "    col for col in train_df.columns if any(\n",
    "        col.startswith(f\"{side}è¶…ç¬¬{rank}ååˆ†é»å‰\") and not col.endswith(\"åˆ¸å•†ä»£è™Ÿ\")\n",
    "        for side in [\"è²·\", \"è³£\"] for rank in range(1, 16)\n",
    "    )\n",
    "]\n",
    "scaler_2 = StandardScaler()\n",
    "X_seq_2 = scaler_2.fit_transform(train_df[seq_cols_2])\n",
    "X_seq_2 = X_seq_2.reshape(len(train_df), 20, -1)\n",
    "X_seq_tensor_2 = torch.tensor(X_seq_2, dtype=torch.float32)\n",
    "model_2 = SimpleLSTM(input_size=X_seq_2.shape[2])\n",
    "with torch.no_grad():\n",
    "    lstm_output_2 = model_2(X_seq_tensor_2).numpy()\n",
    "lstm_cols_2 = [f'LSTM_seq2_embed_{i}' for i in range(lstm_output_2.shape[1])]\n",
    "train_df.drop(columns=[col for col in lstm_cols_2 if col in train_df.columns], inplace=True)\n",
    "lstm_df2 = pd.DataFrame(lstm_output_2, columns=lstm_cols_2)\n",
    "train_df = pd.concat([train_df, lstm_df2], axis=1)\n",
    "\n",
    "# æ¸¬è©¦è³‡æ–™åŒæ­¥è™•ç†\n",
    "X_seq_2_test = scaler_2.transform(test_df[seq_cols_2])\n",
    "X_seq_2_test = X_seq_2_test.reshape(len(test_df), 20, -1)\n",
    "X_seq_tensor_2_test = torch.tensor(X_seq_2_test, dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    lstm_output_2_test = model_2(X_seq_tensor_2_test).numpy()\n",
    "test_df.drop(columns=[col for col in lstm_cols_2 if col in test_df.columns], inplace=True)\n",
    "lstm_df2_test = pd.DataFrame(lstm_output_2_test, columns=lstm_cols_2)\n",
    "test_df = pd.concat([test_df, lstm_df2_test], axis=1)\n",
    "\n",
    "# === ğŸŒ¿ ç‰¹å¾µèˆ‡æ¨™ç±¤åˆ†é›¢ ===\n",
    "target = \"é£†è‚¡\"\n",
    "features = [col for col in train_df.columns if col not in [\"ID\", target]]\n",
    "X = train_df[features]\n",
    "y = train_df[target]\n",
    "\n",
    "print(f\"ğŸ“Š è™•ç†å®Œæˆå¾Œçš„ç‰¹å¾µæ•¸é‡ï¼š{len(features)} å€‹\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"âœ… è³‡æ–™è™•ç†å®Œæˆï¼Œè€—æ™‚ï¼š{end_time - start_time:.2f} ç§’\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d96f66e2-c4f6-4b4a-8c39-f16790cdf353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ è³‡æ–™é è™•ç†é–‹å§‹...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning training data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12736/12736 [00:11<00:00, 1068.92it/s]\n",
      "Processing feature groups: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:02<00:00,  3.95it/s]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 9.53 GiB for an array with shape (12734, 200864) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20460\\300001421.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# === âœ… å»ºç«‹ç‰¹å¾µå„²å­˜æ¡†æ¶ ===\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mtrain_features_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# === ğŸ“Š MA ç‰¹å¾µå·¥ç¨‹ ===\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m   6809\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6810\u001b[0m         \"\"\"\n\u001b[1;32m-> 6811\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6812\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6813\u001b[0m         return self._constructor_from_mgr(data, axes=data.axes).__finalize__(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 604\u001b[1;33m             \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    605\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1786\u001b[0m         \u001b[1;31m#  BlockManager objects not yet attached to a DataFrame.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1787\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_consolidated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1788\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1789\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1790\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_known_consolidated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   2267\u001b[0m     \u001b[0mnew_blocks\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mBlock\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2268\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_blocks\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrouper\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2269\u001b[1;33m         merged_blocks, _ = _merge_blocks(\n\u001b[0m\u001b[0;32m   2270\u001b[0m             \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcan_consolidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2271\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[0;32m   2292\u001b[0m             \u001b[1;31m# Sequence[Union[int, float, complex, str, bytes, generic]],\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2293\u001b[0m             \u001b[1;31m# Sequence[Sequence[Any]], SupportsArray]]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2294\u001b[1;33m             \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2295\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2296\u001b[0m             \u001b[0mbvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mblk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[0marrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcasting\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 9.53 GiB for an array with shape (12734, 200864) and data type float32"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"ğŸ“¦ è³‡æ–™é è™•ç†é–‹å§‹...\")\n",
    "\n",
    "# === ğŸ§¼ ç¼ºå¤±å€¼èˆ‡å‹æ…‹è™•ç† ===\n",
    "for col in tqdm(train_df.columns, desc=\"Cleaning training data\"):\n",
    "    if train_df[col].dtype == 'float64':\n",
    "        train_df[col] = train_df[col].astype('float32').replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# === ğŸ¯ ç‰¹å®šç‰¹å¾µç¾¤è£œå€¼è™•ç† ===\n",
    "feature_groups = [\n",
    "    \"å®˜è‚¡åˆ¸å•†_\", \"å€‹è‚¡åˆ¸å•†åˆ†é»\", \"å€‹è‚¡ä¸»åŠ›è²·è³£è¶…çµ±è¨ˆ\",\n",
    "    \"æ—¥å¤–è³‡_\", \"æ—¥è‡ªç‡Ÿ_\", \"æ—¥æŠ•ä¿¡_\",\n",
    "    \"æŠ€è¡“æŒ‡æ¨™_\", \"æœˆç‡Ÿæ”¶_\", \"å­£IFRSè²¡å ±_\"\n",
    "]\n",
    "for prefix in tqdm(feature_groups, desc=\"Processing feature groups\"):\n",
    "    cols = [col for col in train_df.columns if col.startswith(prefix)]\n",
    "    train_df[cols] = train_df[cols].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# === âœ… å»ºç«‹ç‰¹å¾µå„²å­˜æ¡†æ¶ ===\n",
    "train_features_df = train_df.copy()\n",
    "\n",
    "# === ğŸ“Š MA ç‰¹å¾µå·¥ç¨‹ ===\n",
    "types = [\n",
    "    \"å¼µå¢æ¸›\", \"é‡‘é¡å¢æ¸›(åƒ)\", \"è²·å¼µ\", \"è³£å¼µ\", \"è²·é‡‘é¡(åƒ)\", \"è³£é‡‘é¡(åƒ)\",\n",
    "    \"è²·ç­†æ•¸\", \"è³£ç­†æ•¸\", \"è²·å‡å¼µ\", \"è³£å‡å¼µ\", \"è²·å‡åƒ¹\", \"è³£å‡åƒ¹\", \"è²·å‡å€¼(åƒ)\", \"è³£å‡å€¼(åƒ)\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ“Š ç”Ÿæˆ MA ç‰¹å¾µ\")\n",
    "for rank in tqdm(range(1, 16), desc=\"MA ç‰¹å¾µ\"):\n",
    "    for t in types:\n",
    "        for ma in [5, 10, 20]:\n",
    "            buy_cols = [f\"è²·è¶…ç¬¬{rank}ååˆ†é»å‰{i}å¤©{t}\" for i in range(1, ma + 1)]\n",
    "            sell_cols = [f\"è³£è¶…ç¬¬{rank}ååˆ†é»å‰{i}å¤©{t}\" for i in range(1, ma + 1)]\n",
    "\n",
    "            if all(col in train_df.columns for col in buy_cols):\n",
    "                train_features_df[f\"è²·è¶…ç¬¬{rank}å_{t}_MA{ma}\"] = train_df[buy_cols].mean(axis=1)\n",
    "\n",
    "            if all(col in train_df.columns for col in sell_cols):\n",
    "                train_features_df[f\"è³£è¶…ç¬¬{rank}å_{t}_MA{ma}\"] = train_df[sell_cols].mean(axis=1)\n",
    "\n",
    "# === ğŸ“Š EMA ç‰¹å¾µå·¥ç¨‹ ===\n",
    "print(\"ğŸ“Š ç”Ÿæˆ EMA ç‰¹å¾µ\")\n",
    "for rank in tqdm(range(1, 16), desc=\"EMA ç‰¹å¾µ\"):\n",
    "    for t in types:\n",
    "        for span in [5, 10, 20]:\n",
    "            buy_cols = [f\"è²·è¶…ç¬¬{rank}ååˆ†é»å‰{i}å¤©{t}\" for i in range(1, span + 1)]\n",
    "            sell_cols = [f\"è³£è¶…ç¬¬{rank}ååˆ†é»å‰{i}å¤©{t}\" for i in range(1, span + 1)]\n",
    "\n",
    "            if all(col in train_df.columns for col in buy_cols):\n",
    "                reversed_buy = np.fliplr(train_df[buy_cols].values)\n",
    "                ema_vals = pd.DataFrame(reversed_buy).ewm(span=span, adjust=False).mean().iloc[:, -1].values\n",
    "                train_features_df[f\"è²·è¶…ç¬¬{rank}å_{t}_EMA{span}\"] = ema_vals\n",
    "\n",
    "            if all(col in train_df.columns for col in sell_cols):\n",
    "                reversed_sell = np.fliplr(train_df[sell_cols].values)\n",
    "                ema_vals = pd.DataFrame(reversed_sell).ewm(span=span, adjust=False).mean().iloc[:, -1].values\n",
    "                train_features_df[f\"è³£è¶…ç¬¬{rank}å_{t}_EMA{span}\"] = ema_vals\n",
    "\n",
    "# === ğŸ“Š æŠ€è¡“æŒ‡æ¨™ ===\n",
    "train_features_df['RSI_diff'] = train_df['æŠ€è¡“æŒ‡æ¨™_RSI(10)'].diff().fillna(0)\n",
    "train_features_df['ä¹–é›¢ç‡_change'] = train_df['æŠ€è¡“æŒ‡æ¨™_ä¹–é›¢ç‡(20æ—¥)'].pct_change().fillna(0)\n",
    "\n",
    "# === ğŸ“Š å€‹è‚¡å ±é…¬ç‡èˆ‡æ³¢å‹•åº¦ ===\n",
    "close_cols = [f'å€‹è‚¡å‰{i}å¤©æ”¶ç›¤åƒ¹' for i in range(1, 21)]\n",
    "train_features_df['å€‹è‚¡1å¤©å ±é…¬ç‡'] = (train_df['å€‹è‚¡æ”¶ç›¤åƒ¹'] - train_df['å€‹è‚¡å‰1å¤©æ”¶ç›¤åƒ¹']) / train_df['å€‹è‚¡å‰1å¤©æ”¶ç›¤åƒ¹']\n",
    "train_features_df['å€‹è‚¡5å¤©å ±é…¬ç‡'] = (train_df['å€‹è‚¡æ”¶ç›¤åƒ¹'] - train_df['å€‹è‚¡å‰5å¤©æ”¶ç›¤åƒ¹']) / train_df['å€‹è‚¡å‰5å¤©æ”¶ç›¤åƒ¹']\n",
    "train_features_df['å€‹è‚¡10å¤©å ±é…¬ç‡'] = (train_df['å€‹è‚¡æ”¶ç›¤åƒ¹'] - train_df['å€‹è‚¡å‰10å¤©æ”¶ç›¤åƒ¹']) / train_df['å€‹è‚¡å‰10å¤©æ”¶ç›¤åƒ¹']\n",
    "train_features_df['å€‹è‚¡20å¤©å ±é…¬ç‡'] = (train_df['å€‹è‚¡æ”¶ç›¤åƒ¹'] - train_df['å€‹è‚¡å‰20å¤©æ”¶ç›¤åƒ¹']) / train_df['å€‹è‚¡å‰20å¤©æ”¶ç›¤åƒ¹']\n",
    "train_features_df['å€‹è‚¡5å¤©æ³¢å‹•åº¦'] = train_df[close_cols[:5]].std(axis=1)\n",
    "train_features_df['å€‹è‚¡10å¤©æ³¢å‹•åº¦'] = train_df[close_cols[:10]].std(axis=1)\n",
    "train_features_df['å€‹è‚¡20å¤©æ³¢å‹•åº¦'] = train_df[close_cols].std(axis=1)\n",
    "train_features_df['å€‹è‚¡5å¤©ä¹–é›¢ç‡'] = (train_df['å€‹è‚¡æ”¶ç›¤åƒ¹'] - train_df[close_cols[:5]].mean(axis=1)) / train_df[close_cols[:5]].mean(axis=1)\n",
    "train_features_df['å€‹è‚¡10å¤©ä¹–é›¢ç‡'] = (train_df['å€‹è‚¡æ”¶ç›¤åƒ¹'] - train_df[close_cols[:10]].mean(axis=1)) / train_df[close_cols[:10]].mean(axis=1)\n",
    "train_features_df['å€‹è‚¡19å¤©ä¹–é›¢ç‡'] = (train_df['å€‹è‚¡æ”¶ç›¤åƒ¹'] - train_df[close_cols[:19]].mean(axis=1)) / train_df[close_cols[:19]].mean(axis=1)\n",
    "\n",
    "# === ğŸ“Š æˆäº¤é‡æ³¢å‹•åº¦ ===\n",
    "volume_cols = [f'å€‹è‚¡å‰{i}å¤©æˆäº¤é‡' for i in range(1, 21)]\n",
    "train_features_df['å€‹è‚¡5å¤©æˆäº¤é‡æ³¢å‹•åº¦'] = train_df[volume_cols[:5]].std(axis=1)\n",
    "train_features_df['å€‹è‚¡10å¤©æˆäº¤é‡æ³¢å‹•åº¦'] = train_df[volume_cols[:10]].std(axis=1)\n",
    "train_features_df['å€‹è‚¡20å¤©æˆäº¤é‡æ³¢å‹•åº¦'] = train_df[volume_cols].std(axis=1)\n",
    "\n",
    "# === ğŸ“Š ä¸Šå¸‚åŠ æ¬ŠæŒ‡æ•¸ç‰¹å¾µ ===\n",
    "market_close_cols = [f'ä¸Šå¸‚åŠ æ¬ŠæŒ‡æ•¸å‰{i}å¤©æ”¶ç›¤åƒ¹' for i in range(1, 21)]\n",
    "market_vol_cols = [f'ä¸Šå¸‚åŠ æ¬ŠæŒ‡æ•¸å‰{i}å¤©æˆäº¤é‡' for i in range(1, 21)]\n",
    "train_features_df['ä¸Šå¸‚åŠ æ¬ŠæŒ‡æ•¸1å¤©å ±é…¬ç‡'] = (train_df['ä¸Šå¸‚åŠ æ¬ŠæŒ‡æ•¸æ”¶ç›¤åƒ¹'] - train_df['ä¸Šå¸‚åŠ æ¬ŠæŒ‡æ•¸å‰1å¤©æ”¶ç›¤åƒ¹']) / train_df['ä¸Šå¸‚åŠ æ¬ŠæŒ‡æ•¸å‰1å¤©æ”¶ç›¤åƒ¹']\n",
    "train_features_df['ä¸Šå¸‚åŠ æ¬ŠæŒ‡æ•¸5å¤©å ±é…¬ç‡'] = (train_df['ä¸Šå¸‚åŠ æ¬ŠæŒ‡æ•¸æ”¶ç›¤åƒ¹'] - train_df['ä¸Šå¸‚åŠ æ¬ŠæŒ‡æ•¸å‰5å¤©æ”¶ç›¤åƒ¹']) / train_df['ä¸Šå¸‚åŠ æ¬ŠæŒ‡æ•¸å‰5å¤©æ”¶ç›¤åƒ¹']\n",
    "train_features_df['ä¸Šå¸‚åŠ æ¬ŠæŒ‡æ•¸10å¤©å ±é…¬ç‡'] = (train_df['ä¸Šå¸‚åŠ æ¬ŠæŒ‡æ•¸æ”¶ç›¤åƒ¹'] - train_df['ä¸Šå¸‚åŠ æ¬ŠæŒ‡æ•¸å‰10å¤©æ”¶ç›¤åƒ¹']) / train_df['ä¸Šå¸‚åŠ æ¬ŠæŒ‡æ•¸å‰10å¤©æ”¶ç›¤åƒ¹']\n",
    "train_features_df['ä¸Šå¸‚åŠ æ¬ŠæŒ‡æ•¸20å¤©å ±é…¬ç‡'] = (train_df['ä¸Šå¸‚åŠ æ¬ŠæŒ‡æ•¸æ”¶ç›¤åƒ¹'] - train_df['ä¸Šå¸‚åŠ æ¬ŠæŒ‡æ•¸å‰20å¤©æ”¶ç›¤åƒ¹']) / train_df['ä¸Šå¸‚åŠ æ¬ŠæŒ‡æ•¸å‰20å¤©æ”¶ç›¤åƒ¹']\n",
    "train_features_df['ä¸Šå¸‚åŠ æ¬ŠæŒ‡æ•¸5å¤©æ³¢å‹•åº¦'] = train_df[market_close_cols[:5]].std(axis=1)\n",
    "train_features_df['ä¸Šå¸‚åŠ æ¬ŠæŒ‡æ•¸10å¤©æ³¢å‹•åº¦'] = train_df[market_close_cols[:10]].std(axis=1)\n",
    "train_features_df['ä¸Šå¸‚åŠ æ¬ŠæŒ‡æ•¸20å¤©æ³¢å‹•åº¦'] = train_df[market_close_cols].std(axis=1)\n",
    "train_features_df['ä¸Šå¸‚åŠ æ¬ŠæŒ‡æ•¸5å¤©ä¹–é›¢ç‡'] = (train_df['ä¸Šå¸‚åŠ æ¬ŠæŒ‡æ•¸æ”¶ç›¤åƒ¹'] - train_df[market_close_cols[:5]].mean(axis=1)) / train_df[market_close_cols[:5]].mean(axis=1)\n",
    "train_features_df['ä¸Šå¸‚åŠ æ¬ŠæŒ‡æ•¸10å¤©ä¹–é›¢ç‡'] = (train_df['ä¸Šå¸‚åŠ æ¬ŠæŒ‡æ•¸æ”¶ç›¤åƒ¹'] - train_df[market_close_cols[:10]].mean(axis=1)) / train_df[market_close_cols[:10]].mean(axis=1)\n",
    "train_features_df['ä¸Šå¸‚åŠ æ¬ŠæŒ‡æ•¸19å¤©ä¹–é›¢ç‡'] = (train_df['ä¸Šå¸‚åŠ æ¬ŠæŒ‡æ•¸æ”¶ç›¤åƒ¹'] - train_df[market_close_cols[:19]].mean(axis=1)) / train_df[market_close_cols[:19]].mean(axis=1)\n",
    "train_features_df['ä¸Šå¸‚åŠ æ¬ŠæŒ‡æ•¸5å¤©æˆäº¤é‡æ³¢å‹•åº¦'] = train_df[market_vol_cols[:5]].std(axis=1)\n",
    "train_features_df['ä¸Šå¸‚åŠ æ¬ŠæŒ‡æ•¸10å¤©æˆäº¤é‡æ³¢å‹•åº¦'] = train_df[market_vol_cols[:10]].std(axis=1)\n",
    "train_features_df['ä¸Šå¸‚åŠ æ¬ŠæŒ‡æ•¸20å¤©æˆäº¤é‡æ³¢å‹•åº¦'] = train_df[market_vol_cols].std(axis=1)\n",
    "\n",
    "# === ğŸ¯ ç‰¹å¾µèˆ‡æ¨™ç±¤åˆ†é›¢ ===\n",
    "target = \"é£†è‚¡\"\n",
    "features = [col for col in train_features_df.columns if col not in [\"ID\", target]]\n",
    "X = train_features_df[features]\n",
    "y = train_features_df[target]\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"âœ… è³‡æ–™è™•ç†å®Œæˆï¼Œè€—æ™‚ï¼š{end_time - start_time:.2f} ç§’\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e262c405-28bb-4a09-9558-30fadd42a9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89cde44a-5240-47a4-a80d-218e1373b378",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning training data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11476/11476 [00:00<00:00, 19190.20it/s]\n",
      "Cleaning testing data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10214/10214 [00:00<00:00, 292178.15it/s]\n",
      "C:\\Users\\james\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\training.py:183: UserWarning: [16:55:17] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 40 candidates, totalling 120 fits\n",
      "æœ€ä½³åƒæ•¸: {'subsample': 0.9, 'reg_lambda': 10, 'reg_alpha': 0.1, 'n_estimators': 800, 'max_depth': 8, 'learning_rate': 0.1, 'gamma': 0, 'colsample_bytree': 0.9}\n",
      "æœ€ä½³F1-score: 0.7048661935785835\n",
      "äº¤å‰é©—è­‰ F1-score å¹³å‡å€¼: 0.7367\n",
      "âœ… æˆåŠŸè¼¸å‡º submissionï¼špublic_result1.csv\n"
     ]
    }
   ],
   "source": [
    "#åˆå§‹ç‰ˆæœ¬(å·²å¯«å…¥ç‰¹å¾µå·¥ç¨‹)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import shap\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# === ğŸ§ª åˆ‡åˆ†è³‡æ–™ ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_df[features], y, stratify=y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# === ğŸ”§ è¨­å®šé¡åˆ¥æ¬Šé‡ ===\n",
    "scale_pos_weight = y_train.value_counts()[0] / y_train.value_counts()[1]\n",
    "\n",
    "# === ğŸ“ˆ æ¨¡å‹ç›´æ¥è¨“ç·´ (ä¸ä½¿ç”¨SMOTEæˆ–ROS) ===\n",
    "model = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    gamma=1,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# === ğŸ“Š SHAPé‡è¦ç‰¹å¾µç¯©é¸ï¼ˆä½¿ç”¨æ›´å¤šè³‡æ–™ï¼‰ ===\n",
    "X_sample = X_train.sample(n=2000, random_state=42)\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "shap_importance = pd.DataFrame({\n",
    "    'feature': X_sample.columns,\n",
    "    'mean_abs_shap': np.abs(shap_values).mean(axis=0)\n",
    "}).sort_values(by='mean_abs_shap', ascending=False)\n",
    "\n",
    "top_features = shap_importance.head(50)['feature'].tolist()\n",
    "\n",
    "# === â™»ï¸ ä½¿ç”¨topç‰¹å¾µèˆ‡GridSearchCVé€²è¡Œèª¿åƒ ===\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [600, 800, 1000],\n",
    "    'max_depth': [6, 8, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "    'gamma': [0, 1],\n",
    "    'reg_alpha': [0, 0.1],\n",
    "    'reg_lambda': [1, 5, 10]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=XGBClassifier(\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        eval_metric='logloss',\n",
    "        random_state=42\n",
    "    ),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=100,  # æœå°‹40çµ„åƒæ•¸\n",
    "    scoring='f1',\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(X_train[top_features], y_train)\n",
    "print(\"æœ€ä½³åƒæ•¸:\", random_search.best_params_)\n",
    "print(\"æœ€ä½³F1-score:\", random_search.best_score_)\n",
    "\n",
    "\n",
    "# === ğŸ“ˆ æœ€ä½³æ¨¡å‹äº¤å‰é©—è­‰è©•ä¼° ===\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "f1_scores = cross_val_score(best_model, X_train[top_features], y_train, cv=cv, scoring='f1')\n",
    "print(f\"äº¤å‰é©—è­‰ F1-score å¹³å‡å€¼: {f1_scores.mean():.4f}\")\n",
    "\n",
    "# === ğŸ“¤ æ¸¬è©¦è³‡æ–™é æ¸¬ ===\n",
    "X_public = test_df[top_features].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "test_df[\"é£†è‚¡\"] = best_model.predict(X_public)\n",
    "\n",
    "# === ğŸ’¾ è¼¸å‡º submission çµæœ ===\n",
    "submission = test_df[[\"ID\", \"é£†è‚¡\"]]\n",
    "submission.to_csv(\"public_result1.csv\", index=False, encoding=\"utf-8\", lineterminator='\\n')\n",
    "print(\"âœ… æˆåŠŸè¼¸å‡º submissionï¼špublic_result1.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ea32ea3-17de-42ab-aecd-ab7e6e5d8b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ğŸ¯ å®˜è‚¡åˆ¸å•† & ç±Œç¢¼åˆ†æç‰¹å¾µå·¥ç¨‹ ===\n",
    "\n",
    "# å®˜è‚¡åˆ¸å•†æ¬„ä½\n",
    "gov_cols = [col for col in train_df.columns if \"å®˜è‚¡åˆ¸å•†_\" in col]\n",
    "train_df[gov_cols] = train_df[gov_cols].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# å€‹è‚¡ç±Œç¢¼åˆ†ææ¬„ä½ï¼ˆå«ç†±é–€åº¦ã€è²·è³£åˆ†å¸ƒä¿‚æ•¸ã€åˆ†é»å¯†åº¦ï¼‰\n",
    "chip_cols = [col for col in train_df.columns if \"å€‹è‚¡åˆ¸å•†åˆ†é»\" in col]\n",
    "train_df[chip_cols] = train_df[chip_cols].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# ä¸»åŠ›è²·è³£è¶…çµ±è¨ˆè³‡æ–™\n",
    "main_force_cols = [col for col in train_df.columns if \"å€‹è‚¡ä¸»åŠ›è²·è³£è¶…çµ±è¨ˆ\" in col]\n",
    "train_df[main_force_cols] = train_df[main_force_cols].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# å¤–è³‡ã€è‡ªç‡Ÿã€æŠ•ä¿¡è²·è³£æ¬„ä½\n",
    "inst_cols = [col for col in train_df.columns if col.startswith(\"æ—¥å¤–è³‡_\") or col.startswith(\"æ—¥è‡ªç‡Ÿ_\") or col.startswith(\"æ—¥æŠ•ä¿¡_\")]\n",
    "train_df[inst_cols] = train_df[inst_cols].replace([np.inf, -np.inf], np.nan).fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "061e79fc-e25b-48ba-a6b1-a9f87151128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ğŸ¯ æŠ€è¡“æŒ‡æ¨™ç‰¹å¾µè™•ç† ===\n",
    "\n",
    "# æŠ“å‡ºæ‰€æœ‰æŠ€è¡“æŒ‡æ¨™æ¬„ä½\n",
    "tech_cols = [col for col in train_df.columns if col.startswith(\"æŠ€è¡“æŒ‡æ¨™_\")]\n",
    "\n",
    "# çµ±ä¸€è™•ç†ç¼ºå¤±èˆ‡ç„¡é™å€¼\n",
    "train_df[tech_cols] = train_df[tech_cols].replace([np.inf, -np.inf], np.nan).fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c6aae3b-4a2e-4ee3-801c-d883f29d0242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ğŸ¯ åŸºæœ¬é¢ï¼ˆæœˆç‡Ÿæ”¶ï¼‰ç‰¹å¾µè™•ç† ===\n",
    "\n",
    "# æ‰€æœ‰ä»¥ã€Œæœˆç‡Ÿæ”¶_ã€é–‹é ­çš„æ¬„ä½çµ±ä¸€æŠ“å–\n",
    "fundamental_cols = [col for col in train_df.columns if col.startswith(\"æœˆç‡Ÿæ”¶_\")]\n",
    "\n",
    "# ç¼ºå¤±è™•ç†èˆ‡ç„¡é™å€¼è£œ 0ï¼ˆæŸäº›è²¡å ±æ¬„ä½å¶çˆ¾æœƒç©ºæˆ–é™¤ä»¥ 0ï¼‰\n",
    "train_df[fundamental_cols] = train_df[fundamental_cols].replace([np.inf, -np.inf], np.nan).fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7776d91-4186-41f8-b807-7e6ac31a672c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ğŸ“Š å­£IFRSè²¡å ±ç‰¹å¾µè™•ç† ===\n",
    "\n",
    "# æ“·å–æ‰€æœ‰å­£IFRSè²¡å ±æ¬„ä½\n",
    "ifrs_cols = [col for col in train_df.columns if col.startswith(\"å­£IFRSè²¡å ±_\")]\n",
    "\n",
    "# æ›¿æ› inf èˆ‡ NaN ç‚º 0ï¼ˆé¿å…å ±è¡¨è¨ˆç®—éŒ¯èª¤æˆ–é™¤ä»¥é›¶ï¼‰\n",
    "train_df[ifrs_cols] = train_df[ifrs_cols].replace([np.inf, -np.inf], np.nan).fillna(0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "dl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
